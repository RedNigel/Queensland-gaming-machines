---
title: "Queensland Gaming Exploratory Data Analysis"
author: "Ashley Betts"
date: "11/08/2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

As the EGM data set related to LGA's has no expenditure we're going to have a dig to see if we can find some way to infer it from the other data sets that do contain expenditure. We're after the expenditure so that we can tie it back to demographic type data captured at the LGA by the ABS.

## Data Setup
From the EGM related datasets `all_gambling_df` contains the expenditure and `total_egm_sum` contains the metered wins. We'll summarise and join these to help get a picture of the ratio between expenditure vs wins (payout percentage).
```{r, message=F}
library(knitr)
library(tidyr)
library(dplyr)
library(ggplot2)
source("egm_analysis.R")
all_gamb_sum <- all_gambling_df %>%
  dplyr::select(-game_stream) %>% 
  group_by(x_month_year) %>%
  dplyr::summarise(total_expenditure=sum(player_expenditure))
total_egm_sum <- total_egm_df %>%
  dplyr::select(x_month_year,metered_win) %>%
  group_by(x_month_year) %>%
  dplyr::summarise(total_wins=sum(metered_win))
egm_flow <- full_join(all_gamb_sum,total_egm_sum)
egm_flow <- separate(egm_flow, x_month_year, c("Month", "Year"))
```
Next we determine the payout percentage.
```{r}
egm_flow_sum <- egm_flow %>% group_by(Year) %>% dplyr::summarise(expend=sum(total_expenditure, na.rm = T),pay=sum(total_wins, na.rm=T),paypct=pay/expend)
```
We'll use `t.test` to determine how confident we are that the mean of the payout percentage can be applied to wins to infer the expenditure.
```{r}
tresult <- t.test(mean(egm_flow_sum$paypct)-egm_flow_sum$paypct)
```
We're `r attr(tresult$conf.int,"conf.level")` confident that the mean is within `r tresult$conf.int[1]` and `r tresult$conf.int[2]` of the overall mean.  
Lets take a look at the data to see if there is anything unusual.
```{r}
ggplot(egm_flow_sum, aes(x=Year)) + geom_line(aes(y=expend,col="Expenditure", group=1)) + geom_line(aes(y=pay, col="Winnings", group=1)) + scale_color_manual("",breaks=c("Expenditure", "Winnings"), values=c("red","blue"))
```
Looks like there's something unusual in 2013 happening and possibly 2017 as it drops off. We'll have a closer look.
```{r}
kable(filter(egm_flow, Year == "2013" | Year == "2017"))
```

Yep, missing data in November of 2013 and April 2017. Let's remove it so we can see more realistic confidence intervals.
```{r}
egm_flow_m <- filter(egm_flow, !(Month == "November" & Year == "2013"), !(Month == "April" & Year == "2017"))
egm_flow_sum_m <- egm_flow_m %>% group_by(Year) %>% dplyr::summarise(expend=sum(total_expenditure, na.rm = T),pay=sum(total_wins, na.rm=T),paypct=pay/expend)
tresult <- t.test(mean(egm_flow_sum_m$paypct)-egm_flow_sum_m$paypct)
```
We're `r attr(tresult$conf.int,"conf.level")` confident that the mean is within `r tresult$conf.int[1]` and `r tresult$conf.int[2]` of the overall mean.  
That's pretty tight. Less than 1% so we'll run with a payout percentage of:
```{r}
paypctm <- mean(egm_flow_sum_m$paypct)
```
`r paypctm`

> Note this was generated off of the dataset with the observations that have missing data removed so they didn't skew the mean.

Now we're going to join with some ABS data by LGA region to see if we can pull out any insights.

```{r, message=F}
source("abs_data_by_region.R")
create_abs_dbr_dss(lga_data)
```
We'll tidy up some column names and then bust up the `Month Year` column for joining.
```{r}
names(lg_egm_df) <- c("MonthYear","LGA.Region","Approved.Sites","Operational.Sites","Approved.EGMs", "Operational.EGMs", "Metered.Win")
lga_egm_df <- separate(lg_egm_df, MonthYear, c("Month", "YEAR"))
```
As all of the ABS data is yearly, not monthly average/sum the figures for a year on the EGM LGA dataset.
```{r}
lga_egm_yearly_df <- lga_egm_df %>%
  group_by(LGA.Region,YEAR) %>% 
  dplyr::summarise(avg_app_sites = mean(Approved.Sites), avg_op_sites = mean(Operational.Sites), avg_app_egms = mean(Approved.EGMs), avg_op_egms = mean(Operational.EGMs), tot_metered_win = sum(Metered.Win), tot_inf_expend = tot_metered_win / paypctm)
```
Now we'll grab the mapping between the EGM LGA names and the ABS LGA Codes so we can do some joining.
```{r}
lga_mapping <- create_qld_lga_mapping(lga_pop_ds)
lga_egm_yearly_df <- inner_join(lga_egm_yearly_df,lga_mapping, by=c("LGA.Region"="LABEL"))
```
We can now join on `CODE`. 

We'll check the ABS data for min and max years.
```{r}
absminyr <- min(lga_pop_ds$YEAR)
absmaxyr <- max(lga_pop_ds$YEAR)
```
The ABS data contains data between `r absminyr` and `r absmaxyr`. We need to trim up the EGM data to match.
```{r}
lga_egm_11_15_df <- filter(lga_egm_yearly_df, YEAR >= absminyr, YEAR <= absmaxyr)
```
Next we'll get the population of people who are legally allowed to gamble. Unfortantely the ABS age related data has a range between 15-19 years which includes the 18-19 year olds that can legally gamble. We will include this range to be conservative.

```{r}
lga_gambling_pop_ds <- lga_pop_ds %>%
  mutate(gambling_pop=rowSums(.[54:68])) %>% 
  dplyr::select(CODE,LABEL,YEAR,gambling_pop)
```
We'll now join the population data to the EGM data.
```{r}
lga_egm_11_15_pop_df <- inner_join(lga_egm_11_15_df,lga_gambling_pop_ds)
```
Now lets see how much each eligible gamble person is spending.
```{r}
lga_egm_11_15_pop_df <- mutate(lga_egm_11_15_pop_df, inf_exp_g_person = tot_inf_expend / gambling_pop)
lga_egm_11_15_pop_df %>% dplyr::select(LGA.Region,CODE,YEAR,tot_metered_win,tot_inf_expend,inf_exp_g_person)
```
We can now see the average amount each (remember this is conservative) person who is elible is spending per year.  
Let see which LGA has spent the most on gambling.
```{r}
max_exp_lga <- lga_egm_11_15_pop_df[which.max(lga_egm_11_15_pop_df$inf_exp_g_person),]
```
The LGA that expended the most was `r max_exp_lga$LGA.Region` at `r max_exp_lga$inf_exp_g_person` dollars.
```{r}
kable(ungroup(lga_egm_11_15_pop_df) %>% top_n(50, inf_exp_g_person) %>% dplyr::select(LGA.Region, YEAR, inf_exp_g_person) %>% arrange(desc(inf_exp_g_person)))
```
Now lets check the average expenditure for all LGA's.
```{r}
avg_lga_exp <- mean(lga_egm_11_15_pop_df$inf_exp_g_person, na.rm = T)
```

The average expenditure was `r avg_lga_exp`.

```{r}
lga_data_avl <- lga_egm_11_15_pop_df %>% 
  group_by(LGA.Region) %>% 
  dplyr::summarise(all_na = sum(is.na(inf_exp_g_person)) == n())

lga_with_data <- filter(lga_data_avl, all_na == F)$LGA.Region
lga_egm_11_15_pop_wd_df <- filter(lga_egm_11_15_pop_df, LGA.Region %in% lga_with_data)

dist_tests <- distinct(lga_egm_11_15_pop_wd_df,LGA.Region)

test_against_remaining <- function(x) {
  t1 <- filter(lga_egm_11_15_pop_wd_df,LGA.Region==x[1])$inf_exp_g_person
  t2 <- filter(lga_egm_11_15_pop_wd_df,LGA.Region!=x[1])$inf_exp_g_person
  results <- t.test(t1, t2, paired = F, var.equal = F)
  #results
  list(LGA=x[[1]],p.value=results$p.value,result=results)
}

t <- apply(dist_tests,1,test_against_remaining)
t <- t[order(sapply(t, function(x) x$p.value))]

pvals <- sapply(t, function(x) x$p.value)
t_lga <- sapply(t, function(x) x$LGA)
adj_pvals <- p.adjust(pvals)

sig_adj_pvals <- adj_pvals <= 0.05
# Don't forget to adjust p.values.

```


